import requests
from bs4 import BeautifulSoup
import time
import re
from urllib.parse import urljoin, urlparse
import json
import os
from datetime import datetime
import logging
from typing import Dict, List, Optional
import random

def safe_print(message_with_emoji, message_plain=""):
    """ì•ˆì „í•œ ì¶œë ¥ (Windows ìœ ë‹ˆì½”ë“œ í˜¸í™˜)"""
    try:
        print(message_with_emoji)
    except UnicodeEncodeError:
        print(message_plain if message_plain else message_with_emoji.encode('ascii', 'ignore').decode('ascii'))

class RexResearchCrawler:
    def __init__(self, base_url: str = "https://www.rexresearch.com/invnindx.html"):
        self.base_url = base_url
        self.base_domain = "https://www.rexresearch.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # ë¡œê¹… ì„¤ì • (Windows ìœ ë‹ˆì½”ë“œ í˜¸í™˜)
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('rex_research_crawler.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        # Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
        import sys
        if sys.platform == "win32":
            try:
                # UTF-8 ì½˜ì†” ì¶œë ¥ ì„¤ì • ì‹œë„
                import codecs
                sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
                sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')
            except:
                # ì‹¤íŒ¨ ì‹œ ë¡œê·¸ì—ì„œ ì´ëª¨ì§€ ì œê±° ëª¨ë“œë¡œ ë³€ê²½
                self.use_emoji = False
        else:
            self.use_emoji = True
        self.logger = logging.getLogger(__name__)
        
        # ì´ëª¨ì§€ ì‚¬ìš© ì—¬ë¶€ (Windows í˜¸í™˜ì„±)
        self.use_emoji = True
        
        # ë°ì´í„° ì €ì¥ìš©
        self.inventions_data = []
        self.visited_urls = set()
        self.request_delay = (1, 3)  # 1-3ì´ˆ ëœë¤ ì§€ì—°
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬
        self.output_dir = "rex_inventions"
    
    def _estimate_time(self, num_pages: int) -> int:
        """ì˜ˆìƒ ì†Œìš”ì‹œê°„ ê³„ì‚° (ë¶„ ë‹¨ìœ„)"""
        # í‰ê·  ì§€ì—°ì‹œê°„ + í˜ì´ì§€ ë¡œë“œ ì‹œê°„ ê³ ë ¤
        avg_delay = sum(self.request_delay) / 2
        avg_processing_time = 2  # í˜ì´ì§€ ì²˜ë¦¬ ì‹œê°„
        total_seconds = num_pages * (avg_delay + avg_processing_time)
        return max(1, int(total_seconds / 60))
    
    def safe_log(self, level, message_with_emoji, message_plain):
        """ì•ˆì „í•œ ë¡œê·¸ ì¶œë ¥ (Windows ìœ ë‹ˆì½”ë“œ í˜¸í™˜)"""
        try:
            if hasattr(self, 'use_emoji') and self.use_emoji:
                getattr(self.logger, level)(message_with_emoji)
            else:
                getattr(self.logger, level)(message_plain)
        except UnicodeEncodeError:
            # ì´ëª¨ì§€ ì‚¬ìš© ì‹¤íŒ¨ ì‹œ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ëŒ€ì²´
            self.use_emoji = False
            getattr(self.logger, level)(message_plain)
        
    def create_output_directory(self):
        """ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
            self.safe_log('info', f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±: {self.output_dir}", 
                         f"[CREATE] ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±: {self.output_dir}")
        
    def get_page(self, url: str, retries: int = 3) -> Optional[BeautifulSoup]:
        """í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¤ê³  BeautifulSoup ê°ì²´ë¡œ ë°˜í™˜"""
        for attempt in range(retries):
            try:
                time.sleep(random.uniform(*self.request_delay))
                
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                
                # ì¸ì½”ë”© ì²˜ë¦¬
                if response.encoding is None:
                    response.encoding = 'utf-8'
                elif response.encoding.lower() in ['iso-8859-1', 'windows-1252']:
                    response.encoding = 'utf-8'
                
                soup = BeautifulSoup(response.text, 'html.parser')
                self.safe_log('info', f"âœ… í˜ì´ì§€ ë¡œë“œ ì„±ê³µ: {url}", f"[SUCCESS] í˜ì´ì§€ ë¡œë“œ ì„±ê³µ: {url}")
                return soup
                
            except requests.RequestException as e:
                self.safe_log('warning', f"âš ï¸ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{retries}): {url} - {e}", 
                            f"[WARNING] í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{retries}): {url} - {e}")
                if attempt < retries - 1:
                    time.sleep(2 ** attempt)
                    
        self.safe_log('error', f"âŒ ëª¨ë“  ì‹œë„ ì‹¤íŒ¨: {url}", f"[ERROR] ëª¨ë“  ì‹œë„ ì‹¤íŒ¨: {url}")
        return None
    
    def extract_invention_links(self, soup: BeautifulSoup) -> List[Dict[str, str]]:
        """ë©”ì¸ í˜ì´ì§€ì—ì„œ ë°œëª…í’ˆ/ë°œëª…ì ë§í¬ë“¤ì„ ì¶”ì¶œ"""
        invention_links = []
        
        # ëª¨ë“  ë§í¬ ì°¾ê¸°
        links = soup.find_all('a', href=True)
        
        for link in links:
            href = link.get('href', '').strip()
            text = link.get_text(strip=True)
            
            if not href or not text:
                continue
                
            # ë°œëª…í’ˆ/ë°œëª…ì ë§í¬ì¸ì§€ íŒë‹¨
            if self.is_invention_link(href, text):
                full_url = urljoin(self.base_domain, href)
                
                invention_links.append({
                    'name': text,
                    'url': full_url,
                    'href': href,
                    'category': self.get_link_category(text)
                })
        
        # ì¤‘ë³µ ì œê±°
        seen_urls = set()
        unique_links = []
        for link in invention_links:
            if link['url'] not in seen_urls:
                seen_urls.add(link['url'])
                unique_links.append(link)
        
        self.safe_log('info', f"ğŸ“‹ ì´ {len(unique_links)}ê°œì˜ ë°œëª…í’ˆ ë§í¬ ë°œê²¬", 
                     f"[INFO] ì´ {len(unique_links)}ê°œì˜ ë°œëª…í’ˆ ë§í¬ ë°œê²¬")
        return unique_links
    
    def is_invention_link(self, href: str, text: str) -> bool:
        """ë§í¬ê°€ ë°œëª…í’ˆ/ë°œëª…ì ê´€ë ¨ì¸ì§€ íŒë‹¨"""
        href_lower = href.lower()
        text_lower = text.lower()
        
        # ì œì™¸í•  íŒ¨í„´ë“¤
        exclude_patterns = [
            'javascript:', 'mailto:', '#', 'http://', 'https://',
            'index.html', 'home.html', 'about.html', 'contact.html',
            'search.html', 'links.html', 'disclaimer.html'
        ]
        
        exclude_texts = [
            'home', 'back', 'top', 'index', 'search', 'contact',
            'about', 'links', 'disclaimer', 'rexresearch',
            'inventor index', 'subject index'
        ]
        
        # ì œì™¸ íŒ¨í„´ ì²´í¬
        for pattern in exclude_patterns:
            if pattern in href_lower:
                return False
        
        for pattern in exclude_texts:
            if pattern in text_lower:
                return False
        
        # ìœ íš¨í•œ ë§í¬ ì¡°ê±´
        return (
            href.endswith('.html') and 
            len(text) > 2 and 
            not text.isdigit() and
            not text.startswith('[') and
            not text.startswith('(') and
            len(text) < 200  # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ ì œì™¸
        )
    
    def get_link_category(self, text: str) -> str:
        """ë§í¬ì˜ ì¹´í…Œê³ ë¦¬ ì¶”ì •"""
        text_lower = text.lower()
        
        # ì—ë„ˆì§€ ê´€ë ¨
        energy_keywords = ['energy', 'power', 'electric', 'magnetic', 'battery', 'fuel', 'solar', 'generator']
        if any(keyword in text_lower for keyword in energy_keywords):
            return 'energy'
        
        # ì˜ë£Œ ê´€ë ¨
        medical_keywords = ['medical', 'health', 'therapy', 'healing', 'cure', 'treatment']
        if any(keyword in text_lower for keyword in medical_keywords):
            return 'medical'
        
        # êµí†µ ê´€ë ¨
        transport_keywords = ['car', 'vehicle', 'engine', 'motor', 'aviation', 'aircraft']
        if any(keyword in text_lower for keyword in transport_keywords):
            return 'transport'
        
        return 'general'
    
    def clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text:
            return ""
        
        # HTML ì—”í‹°í‹° ì²˜ë¦¬
        text = text.replace('&nbsp;', ' ')
        text = text.replace('&amp;', '&')
        text = text.replace('&lt;', '<')
        text = text.replace('&gt;', '>')
        text = text.replace('&quot;', '"')
        text = text.replace('&apos;', "'")
        
        # ì—¬ëŸ¬ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        return text.strip()
    
    def extract_invention_content(self, soup: BeautifulSoup, url: str, name: str) -> Dict:
        """ê°œë³„ ë°œëª…í’ˆ í˜ì´ì§€ì—ì„œ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ"""
        invention_data = {
            'name': name,
            'url': url,
            'extracted_at': datetime.now().isoformat(),
            'title': '',
            'description': '',
            'principle': '',
            'technical_details': [],
            'patents': [],
            'images': [],
            'diagrams': [],
            'references': [],
            'full_content': '',
            'structured_sections': {}
        }
        
        try:
            # í˜ì´ì§€ ì œëª©
            title_tag = soup.find('title')
            if title_tag:
                invention_data['title'] = self.clean_text(title_tag.get_text())
            
            # ë©”íƒ€ ì„¤ëª…
            meta_desc = soup.find('meta', {'name': 'description'})
            if meta_desc:
                invention_data['meta_description'] = meta_desc.get('content', '')
            
            # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            full_text = soup.get_text(separator='\n', strip=True)
            invention_data['full_content'] = self.clean_text(full_text)
            
            # êµ¬ì¡°í™”ëœ ì„¹ì…˜ ì¶”ì¶œ
            self.extract_structured_sections(soup, invention_data)
            
            # ì´ë¯¸ì§€ ë° ë‹¤ì´ì–´ê·¸ë¨ ì¶”ì¶œ
            self.extract_images_and_diagrams(soup, url, invention_data)
            
            # íŠ¹í—ˆ ì •ë³´ ì¶”ì¶œ
            self.extract_patent_info(full_text, invention_data)
            
            # ì°¸ì¡° ë§í¬ ì¶”ì¶œ
            self.extract_references(soup, invention_data)
            
            # ê¸°ìˆ ì  ì›ë¦¬ ì¶”ì¶œ
            self.extract_technical_principle(soup, invention_data)
            
        except Exception as e:
            self.safe_log('error', f"âŒ ì»¨í…ì¸  ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {url} - {e}", 
                         f"[ERROR] ì»¨í…ì¸  ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {url} - {e}")
        
        return invention_data
    
    def extract_structured_sections(self, soup: BeautifulSoup, invention_data: Dict):
        """êµ¬ì¡°í™”ëœ ì„¹ì…˜ ì¶”ì¶œ"""
        sections = {}
        
        # í—¤ë”©ìœ¼ë¡œ ì„¹ì…˜ êµ¬ë¶„
        headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
        
        for heading in headings:
            heading_text = self.clean_text(heading.get_text())
            if heading_text:
                # í—¤ë”© ë‹¤ìŒì˜ ì»¨í…ì¸  ì¶”ì¶œ
                content = []
                current = heading.next_sibling
                
                while current and current.name not in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                    if hasattr(current, 'get_text'):
                        text = self.clean_text(current.get_text())
                        if text and len(text) > 10:
                            content.append(text)
                    current = current.next_sibling
                    if not current:
                        break
                
                if content:
                    sections[heading_text] = content
        
        invention_data['structured_sections'] = sections
        
        # ì£¼ìš” ë¬¸ë‹¨ë“¤ ì¶”ì¶œ
        paragraphs = soup.find_all('p')
        tech_details = []
        for p in paragraphs:
            text = self.clean_text(p.get_text())
            if text and len(text) > 50:  # ì˜ë¯¸ìˆëŠ” ê¸¸ì´ì˜ ë¬¸ë‹¨ë§Œ
                tech_details.append(text)
        
        invention_data['technical_details'] = tech_details
    
    def extract_images_and_diagrams(self, soup: BeautifulSoup, base_url: str, invention_data: Dict):
        """ì´ë¯¸ì§€ ë° ë‹¤ì´ì–´ê·¸ë¨ ì •ë³´ ì¶”ì¶œ"""
        images = soup.find_all('img')
        
        for img in images:
            src = img.get('src', '')
            alt = img.get('alt', '')
            title = img.get('title', '')
            
            if src:
                full_img_url = urljoin(base_url, src)
                filename = os.path.basename(src)
                
                img_info = {
                    'url': full_img_url,
                    'filename': filename,
                    'alt_text': self.clean_text(alt),
                    'title': self.clean_text(title),
                    'type': self.classify_image_type(filename, alt)
                }
                
                if img_info['type'] == 'diagram':
                    invention_data['diagrams'].append(img_info)
                else:
                    invention_data['images'].append(img_info)
    
    def classify_image_type(self, filename: str, alt_text: str) -> str:
        """ì´ë¯¸ì§€ íƒ€ì… ë¶„ë¥˜"""
        filename_lower = filename.lower()
        alt_lower = alt_text.lower()
        
        diagram_keywords = ['diagram', 'schematic', 'circuit', 'blueprint', 'plan', 'design']
        photo_keywords = ['photo', 'picture', 'image']
        
        if any(keyword in filename_lower or keyword in alt_lower for keyword in diagram_keywords):
            return 'diagram'
        elif any(keyword in filename_lower or keyword in alt_lower for keyword in photo_keywords):
            return 'photo'
        else:
            return 'image'
    
    def extract_patent_info(self, text: str, invention_data: Dict):
        """íŠ¹í—ˆ ì •ë³´ ì¶”ì¶œ"""
        patent_patterns = [
            r'(?:US\s*)?Patent\s+(?:No\.?\s*)?(\d{1,2}[,.\s]?\d{3}[,.\s]?\d{3})',
            r'U\.S\.\s*Patent\s+(\d{1,2}[,.\s]?\d{3}[,.\s]?\d{3})',
            r'Patent\s+#\s*(\d{1,2}[,.\s]?\d{3}[,.\s]?\d{3})',
            r'Pat\.\s*No\.\s*(\d{1,2}[,.\s]?\d{3}[,.\s]?\d{3})'
        ]
        
        patents = set()
        for pattern in patent_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                clean_patent = re.sub(r'[,.\s]', '', match)
                if len(clean_patent) >= 6:  # ìœ íš¨í•œ íŠ¹í—ˆ ë²ˆí˜¸ ê¸¸ì´
                    patents.add(clean_patent)
        
        invention_data['patents'] = list(patents)
    
    def extract_references(self, soup: BeautifulSoup, invention_data: Dict):
        """ì°¸ì¡° ë§í¬ ì¶”ì¶œ"""
        links = soup.find_all('a', href=True)
        references = []
        
        for link in links:
            href = link.get('href', '')
            text = self.clean_text(link.get_text())
            
            if href and text and href.startswith('http'):
                references.append({
                    'url': href,
                    'text': text
                })
        
        invention_data['references'] = references
    
    def extract_technical_principle(self, soup: BeautifulSoup, invention_data: Dict):
        """ê¸°ìˆ ì  ì›ë¦¬ ë° ì„¤ëª… ì¶”ì¶œ"""
        # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì›ë¦¬ ê´€ë ¨ ë¶€ë¶„ ì°¾ê¸°
        full_text = invention_data['full_content']
        
        # ì›ë¦¬ ê´€ë ¨ í‚¤ì›Œë“œë¡œ ì„¹ì…˜ ì°¾ê¸°
        principle_keywords = [
            'principle', 'theory', 'mechanism', 'operation', 'working',
            'function', 'process', 'method', 'technique', 'approach'
        ]
        
        # ë¬¸ë‹¨ë³„ë¡œ ë¶„ì„í•˜ì—¬ ì›ë¦¬ ì„¤ëª… ì¶”ì¶œ
        paragraphs = invention_data['technical_details']
        principle_paragraphs = []
        
        for paragraph in paragraphs:
            paragraph_lower = paragraph.lower()
            if any(keyword in paragraph_lower for keyword in principle_keywords):
                principle_paragraphs.append(paragraph)
        
        if principle_paragraphs:
            invention_data['principle'] = '\n\n'.join(principle_paragraphs[:3])  # ì²˜ìŒ 3ê°œ ë¬¸ë‹¨
        else:
            # ì›ë¦¬ ì„¤ëª…ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì˜ë¯¸ìˆëŠ” ë¬¸ë‹¨ì„ ì„¤ëª…ìœ¼ë¡œ
            if paragraphs:
                invention_data['description'] = paragraphs[0]
    
    def save_invention_file(self, invention_data: Dict) -> str:
        """ê°œë³„ ë°œëª…í’ˆ íŒŒì¼ ì €ì¥ (LLM í•™ìŠµìš© í˜•ì‹)"""
        # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
        name = invention_data.get('name', 'unknown')
        safe_name = re.sub(r'[^\w\s-]', '', name).strip()
        safe_name = re.sub(r'[-\s]+', '_', safe_name)
        safe_name = safe_name[:100]  # íŒŒì¼ëª… ê¸¸ì´ ì œí•œ
        
        filename = f"{safe_name}.txt"
        filepath = os.path.join(self.output_dir, filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                # LLM í•™ìŠµìš© êµ¬ì¡°í™”ëœ í˜•ì‹
                f.write("=" * 80 + "\n")
                f.write(f"INVENTION: {invention_data.get('name', 'Unknown')}\n")
                f.write("=" * 80 + "\n\n")
                
                # ê¸°ë³¸ ì •ë³´
                f.write("BASIC INFORMATION:\n")
                f.write("-" * 40 + "\n")
                f.write(f"Title: {invention_data.get('title', 'N/A')}\n")
                f.write(f"URL: {invention_data.get('url', 'N/A')}\n")
                f.write(f"Extraction Date: {invention_data.get('extracted_at', 'N/A')}\n\n")
                
                # íŠ¹í—ˆ ì •ë³´
                patents = invention_data.get('patents', [])
                if patents:
                    f.write("PATENT INFORMATION:\n")
                    f.write("-" * 40 + "\n")
                    for patent in patents:
                        f.write(f"Patent Number: {patent}\n")
                    f.write("\n")
                
                # ê¸°ìˆ ì  ì›ë¦¬
                principle = invention_data.get('principle', '')
                if principle:
                    f.write("TECHNICAL PRINCIPLE:\n")
                    f.write("-" * 40 + "\n")
                    f.write(f"{principle}\n\n")
                
                # ìƒì„¸ ì„¤ëª…
                description = invention_data.get('description', '')
                if description:
                    f.write("DESCRIPTION:\n")
                    f.write("-" * 40 + "\n")
                    f.write(f"{description}\n\n")
                
                # ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­
                tech_details = invention_data.get('technical_details', [])
                if tech_details:
                    f.write("TECHNICAL DETAILS:\n")
                    f.write("-" * 40 + "\n")
                    for i, detail in enumerate(tech_details[:5], 1):  # ì²˜ìŒ 5ê°œë§Œ
                        f.write(f"{i}. {detail}\n\n")
                
                # êµ¬ì¡°í™”ëœ ì„¹ì…˜ë“¤
                sections = invention_data.get('structured_sections', {})
                if sections:
                    f.write("STRUCTURED SECTIONS:\n")
                    f.write("-" * 40 + "\n")
                    for section_title, content_list in sections.items():
                        f.write(f"\n[{section_title}]\n")
                        for content in content_list[:3]:  # ê° ì„¹ì…˜ë‹¹ 3ê°œê¹Œì§€
                            f.write(f"{content}\n")
                    f.write("\n")
                
                # ì´ë¯¸ì§€ ë° ë‹¤ì´ì–´ê·¸ë¨ ì •ë³´
                images = invention_data.get('images', [])
                diagrams = invention_data.get('diagrams', [])
                
                if images or diagrams:
                    f.write("VISUAL MATERIALS:\n")
                    f.write("-" * 40 + "\n")
                    
                    if diagrams:
                        f.write("Diagrams and Schematics:\n")
                        for i, diag in enumerate(diagrams, 1):
                            f.write(f"{i}. File: {diag['filename']}\n")
                            f.write(f"   URL: {diag['url']}\n")
                            if diag['alt_text']:
                                f.write(f"   Description: {diag['alt_text']}\n")
                            if diag['title']:
                                f.write(f"   Title: {diag['title']}\n")
                            f.write("\n")
                    
                    if images:
                        f.write("Images and Photos:\n")
                        for i, img in enumerate(images, 1):
                            f.write(f"{i}. File: {img['filename']}\n")
                            f.write(f"   URL: {img['url']}\n")
                            if img['alt_text']:
                                f.write(f"   Description: {img['alt_text']}\n")
                            if img['title']:
                                f.write(f"   Title: {img['title']}\n")
                            f.write("\n")
                
                # ì°¸ì¡° ìë£Œ
                references = invention_data.get('references', [])
                if references:
                    f.write("REFERENCES:\n")
                    f.write("-" * 40 + "\n")
                    for i, ref in enumerate(references[:10], 1):  # ì²˜ìŒ 10ê°œë§Œ
                        f.write(f"{i}. {ref['text']}\n")
                        f.write(f"   URL: {ref['url']}\n\n")
                
                # ì „ì²´ ì»¨í…ì¸  (LLM í•™ìŠµìš©)
                f.write("FULL CONTENT FOR AI TRAINING:\n")
                f.write("-" * 40 + "\n")
                f.write(invention_data.get('full_content', ''))
                f.write("\n\n")
                
                # ë©”íƒ€ë°ì´í„° (JSON í˜•ì‹)
                f.write("METADATA (JSON):\n")
                f.write("-" * 40 + "\n")
                metadata = {
                    'name': invention_data.get('name'),
                    'title': invention_data.get('title'),
                    'url': invention_data.get('url'),
                    'patents': invention_data.get('patents'),
                    'image_count': len(invention_data.get('images', [])),
                    'diagram_count': len(invention_data.get('diagrams', [])),
                    'reference_count': len(invention_data.get('references', [])),
                    'extracted_at': invention_data.get('extracted_at')
                }
                f.write(json.dumps(metadata, indent=2, ensure_ascii=False))
            
            self.safe_log('info', f"ğŸ’¾ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filepath}", f"[SAVE] íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filepath}")
            return filepath
            
        except Exception as e:
            self.safe_log('error', f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {filepath} - {e}", 
                         f"[ERROR] íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {filepath} - {e}")
            return None
    
    def crawl_invention_page(self, invention_info: Dict) -> Optional[Dict]:
        """ê°œë³„ ë°œëª…í’ˆ í˜ì´ì§€ í¬ë¡¤ë§"""
        url = invention_info['url']
        name = invention_info['name']
        
        if url in self.visited_urls:
            return None
        
        self.visited_urls.add(url)
        
        soup = self.get_page(url)
        if not soup:
            return None
        
        invention_data = self.extract_invention_content(soup, url, name)
        invention_data['category'] = invention_info.get('category', 'general')
        
        return invention_data
    
    def run_crawler(self, max_pages: int = 0):
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        self.safe_log('info', f"ğŸš€ Rex Research í¬ë¡¤ë§ ì‹œì‘: {self.base_url}", 
                     f"[START] Rex Research í¬ë¡¤ë§ ì‹œì‘: {self.base_url}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.create_output_directory()
        
        # ë©”ì¸ í˜ì´ì§€ ë¡œë“œ
        main_soup = self.get_page(self.base_url)
        if not main_soup:
            self.safe_log('error', "âŒ ë©”ì¸ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨", "[ERROR] ë©”ì¸ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨")
            return None
        
        # ë°œëª…í’ˆ ë§í¬ ì¶”ì¶œ
        invention_links = self.extract_invention_links(main_soup)
        
        if not invention_links:
            self.safe_log('warning', "âš ï¸ ë°œëª…í’ˆ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "[WARNING] ë°œëª…í’ˆ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None
        
        # ì „ì²´ ë§í¬ ìˆ˜ ë¡œê·¸
        total_found = len(invention_links)
        self.safe_log('info', f"ğŸ“Š ì´ {total_found}ê°œì˜ ë°œëª…í’ˆ ë§í¬ ë°œê²¬!", 
                     f"[INFO] ì´ {total_found}ê°œì˜ ë°œëª…í’ˆ ë§í¬ ë°œê²¬!")
        
        # í˜ì´ì§€ ìˆ˜ ì œí•œ
        if max_pages > 0:
            invention_links = invention_links[:max_pages]
            self.safe_log('info', f"ğŸ”¢ ì²˜ìŒ {max_pages}ê°œë§Œ í¬ë¡¤ë§í•©ë‹ˆë‹¤", 
                         f"[LIMIT] ì²˜ìŒ {max_pages}ê°œë§Œ í¬ë¡¤ë§í•©ë‹ˆë‹¤")
        else:
            self.safe_log('info', f"ğŸŒŸ ì „ì²´ {total_found}ê°œ ë°œëª…í’ˆì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤!", 
                         f"[FULL] ì „ì²´ {total_found}ê°œ ë°œëª…í’ˆì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤!")
        
        self.safe_log('info', f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘ - ì˜ˆìƒ ì†Œìš”ì‹œê°„: {self._estimate_time(len(invention_links))}ë¶„", 
                     f"[START] í¬ë¡¤ë§ ì‹œì‘ - ì˜ˆìƒ ì†Œìš”ì‹œê°„: {self._estimate_time(len(invention_links))}ë¶„")
        
        saved_files = []
        success_count = 0
        
        # ê° ë°œëª…í’ˆ í˜ì´ì§€ í¬ë¡¤ë§
        for i, invention_info in enumerate(invention_links, 1):
            clean_name = invention_info['name'][:50].replace('\n', ' ').strip()
            self.safe_log('info', f"ğŸ”„ ì§„í–‰ìƒí™©: {i}/{len(invention_links)} - {clean_name}", 
                         f"[PROGRESS] {i}/{len(invention_links)} - {clean_name}")
            
            try:
                invention_data = self.crawl_invention_page(invention_info)
                if invention_data:
                    self.inventions_data.append(invention_data)
                    
                    # ê°œë³„ íŒŒì¼ ì €ì¥
                    saved_file = self.save_invention_file(invention_data)
                    if saved_file:
                        saved_files.append(saved_file)
                        success_count += 1
                    
            except KeyboardInterrupt:
                self.safe_log('info', "â¹ï¸ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤", "[STOP] ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤")
                break
            except Exception as e:
                self.safe_log('error', f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {invention_info['name']} - {e}", 
                             f"[ERROR] í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {invention_info['name']} - {e}")
                continue
        
        # ìµœì¢… ê²°ê³¼
        result = {
            'total_links': len(invention_links),
            'successful_crawls': success_count,
            'saved_files': saved_files,
            'output_directory': self.output_dir,
            'failed_count': len(invention_links) - success_count
        }
        
        self.safe_log('info', f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! ì„±ê³µ: {success_count}/{len(invention_links)}", 
                     f"[COMPLETE] í¬ë¡¤ë§ ì™„ë£Œ! ì„±ê³µ: {success_count}/{len(invention_links)}")
        return result

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    crawler = RexResearchCrawler()
    
    safe_print("ğŸ•·ï¸ Rex Research ë°œëª…í’ˆ í¬ë¡¤ëŸ¬ ì‹œì‘", "Rex Research ë°œëª…í’ˆ í¬ë¡¤ëŸ¬ ì‹œì‘")
    safe_print("ì „ì²´ ë°œëª…í’ˆ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    safe_print("âš ï¸ ëŒ€ìš©ëŸ‰ í¬ë¡¤ë§ì…ë‹ˆë‹¤. ì¤‘ë‹¨ ì‹œ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”")
    
    result = crawler.run_crawler(max_pages=0)  # ì „ì²´ í¬ë¡¤ë§
    
    if result:
        safe_print(f"\nâœ… í¬ë¡¤ë§ ê²°ê³¼:", "\n[RESULTS] í¬ë¡¤ë§ ê²°ê³¼:")
        safe_print(f"   - ì´ ë§í¬ ìˆ˜: {result['total_links']}")
        safe_print(f"   - ì„±ê³µì  í¬ë¡¤ë§: {result['successful_crawls']}")
        safe_print(f"   - ì‹¤íŒ¨ ìˆ˜: {result['failed_count']}")
        safe_print(f"   - ì €ì¥ëœ íŒŒì¼: {len(result['saved_files'])}ê°œ")
        safe_print(f"   - ì¶œë ¥ ë””ë ‰í† ë¦¬: {result['output_directory']}/")
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        if crawler.inventions_data:
            categories = {}
            for invention in crawler.inventions_data:
                cat = invention.get('category', 'general')
                categories[cat] = categories.get(cat, 0) + 1
            
            safe_print(f"\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ í†µê³„:", "\n[CATEGORY STATS] ì¹´í…Œê³ ë¦¬ë³„ í†µê³„:")
            for cat, count in categories.items():
                safe_print(f"   - {cat}: {count}ê°œ")
    else:
        safe_print("âŒ í¬ë¡¤ë§ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", "[ERROR] í¬ë¡¤ë§ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

def safe_print(message_with_emoji, message_plain=""):
    """ì•ˆì „í•œ ì¶œë ¥ (Windows ìœ ë‹ˆì½”ë“œ í˜¸í™˜)"""
    try:
        print(message_with_emoji)
    except UnicodeEncodeError:
        print(message_plain if message_plain else message_with_emoji.encode('ascii', 'ignore').decode('ascii'))