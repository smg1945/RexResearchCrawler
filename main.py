#!/usr/bin/env python3
"""
Rex Research í¬ë¡¤ëŸ¬ ê°„ë‹¨ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ì‚¬ìš©ë²•: python run_crawler.py [ì˜µì…˜]
"""

import argparse
import sys
import os
from run_crawler import RexResearchCrawler

def main():
    parser = argparse.ArgumentParser(
        description='Rex Research ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ í”„ë¡œê·¸ë¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python run_crawler.py                    # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ 50í˜ì´ì§€ í¬ë¡¤ë§
  python run_crawler.py -n 20              # 20í˜ì´ì§€ë§Œ í¬ë¡¤ë§
  python run_crawler.py -d 2 5             # 2-5ì´ˆ ì§€ì—°ìœ¼ë¡œ í¬ë¡¤ë§
  python run_crawler.py -o my_data         # ì¶œë ¥ íŒŒì¼ëª… ì§€ì •
  python run_crawler.py --verbose          # ìƒì„¸ ë¡œê·¸ ì¶œë ¥
        """
    )
    
    parser.add_argument(
        '-n', '--max-pages',
        type=int,
        default=50,
        help='í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 50)'
    )
    
    parser.add_argument(
        '-d', '--delay',
        nargs=2,
        type=float,
        default=[1.0, 3.0],
        metavar=('MIN', 'MAX'),
        help='ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„ ë²”ìœ„ (ì´ˆ) (ê¸°ë³¸ê°’: 1.0 3.0)'
    )
    
    parser.add_argument(
        '-o', '--output',
        type=str,
        default='rex_research_data',
        help='ì¶œë ¥ íŒŒì¼ëª… ì ‘ë‘ì‚¬ (ê¸°ë³¸ê°’: rex_research_data)'
    )
    
    parser.add_argument(
        '--url',
        type=str,
        default='https://www.rexresearch.com/invnindx.html',
        help='í¬ë¡¤ë§í•  ê¸°ë³¸ URL'
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='ìƒì„¸ ë¡œê·¸ ì¶œë ¥'
    )
    
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='ì‹¤ì œ í¬ë¡¤ë§ ì—†ì´ ë§í¬ë§Œ í™•ì¸'
    )
    
    args = parser.parse_args()
    
    # ë¡œê¹… ë ˆë²¨ ì„¤ì •
    if args.verbose:
        import logging
        logging.getLogger().setLevel(logging.DEBUG)
    
    print("=" * 60)
    print("ğŸ•·ï¸  Rex Research í¬ë¡¤ëŸ¬")
    print("=" * 60)
    print(f"ğŸ“‹ ì„¤ì •:")
    print(f"   - ìµœëŒ€ í˜ì´ì§€: {args.max_pages}")
    print(f"   - ìš”ì²­ ì§€ì—°: {args.delay[0]}-{args.delay[1]}ì´ˆ")
    print(f"   - ì¶œë ¥ íŒŒì¼: {args.output}_*.csv/json")
    print(f"   - ê¸°ë³¸ URL: {args.url}")
    print(f"   - Dry Run: {'ì˜ˆ' if args.dry_run else 'ì•„ë‹ˆì˜¤'}")
    print("=" * 60)
    
    try:
        # í¬ë¡¤ëŸ¬ ìƒì„± ë° ì„¤ì •
        crawler = RexResearchCrawler(base_url=args.url)
        crawler.request_delay = tuple(args.delay)
        
        if args.dry_run:
            print("ğŸ” Dry Run ëª¨ë“œ: ë§í¬ë§Œ í™•ì¸í•©ë‹ˆë‹¤...")
            main_soup = crawler.get_page(args.url)
            if main_soup:
                links = crawler.extract_inventor_links(main_soup)
                print(f"âœ… ì´ {len(links)}ê°œì˜ ë°œëª…ì ë§í¬ ë°œê²¬")
                
                print("\nğŸ“ ë°œê²¬ëœ ë§í¬ë“¤ (ì²˜ìŒ 10ê°œ):")
                for i, link in enumerate(links[:10], 1):
                    print(f"   {i:2d}. {link['name'][:50]:<50} -> {link['href']}")
                
                if len(links) > 10:
                    print(f"   ... ê·¸ë¦¬ê³  {len(links) - 10}ê°œ ë”")
            else:
                print("âŒ ë©”ì¸ í˜ì´ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰
        print("ğŸš€ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        result = crawler.run_crawler(max_pages=args.max_pages)
        
        if result:
            print("\nâœ… í¬ë¡¤ë§ ì™„ë£Œ!")
            print(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
            print(f"   - ë°œëª…ì ìˆ˜: {len(crawler.inventors_data)}")
            
            total_inventions = sum(len(inv.get('inventions', [])) for inv in crawler.inventors_data)
            total_patents = sum(len(inv.get('patents', [])) for inv in crawler.inventors_data)
            total_images = sum(len(inv.get('images', [])) for inv in crawler.inventors_data)
            
            print(f"   - ë°œëª…í’ˆ ìˆ˜: {total_inventions}")
            print(f"   - íŠ¹í—ˆ ìˆ˜: {total_patents}")
            print(f"   - ì´ë¯¸ì§€ ìˆ˜: {total_images}")
            
            print(f"\nğŸ“ ì €ì¥ëœ íŒŒì¼:")
            for file_type, filename in result.items():
                file_size = os.path.getsize(filename) / 1024  # KB ë‹¨ìœ„
                print(f"   - {filename} ({file_size:.1f} KB)")
            
            # ìƒìœ„ ë°œëª…ì í‘œì‹œ
            if crawler.inventors_data:
                top_inventors = sorted(
                    crawler.inventors_data,
                    key=lambda x: len(x.get('inventions', [])),
                    reverse=True
                )[:5]
                
                print(f"\nğŸ† ìƒìœ„ 5ê°œ ë°œëª…ì (ë°œëª…í’ˆ ìˆ˜):")
                for i, inventor in enumerate(top_inventors, 1):
                    inv_count = len(inventor.get('inventions', []))
                    pat_count = len(inventor.get('patents', []))
                    print(f"   {i}. {inventor.get('name', 'Unknown')[:40]:<40} "
                          f"ë°œëª…í’ˆ: {inv_count:2d}, íŠ¹í—ˆ: {pat_count:2d}")
        else:
            print("âŒ í¬ë¡¤ë§ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            print("ğŸ“‹ ë¡œê·¸ íŒŒì¼(rex_research_crawler.log)ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        if hasattr(crawler, 'inventors_data') and crawler.inventors_data:
            print("ğŸ’¾ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤...")
            try:
                result = crawler.save_data(args.output + "_interrupted")
                print(f"âœ… ë¶€ë¶„ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {result}")
            except Exception as e:
                print(f"âŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
        sys.exit(0)
        
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ğŸ“‹ ë¡œê·¸ íŒŒì¼(rex_research_crawler.log)ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        sys.exit(1)

if __name__ == "__main__":
    main()